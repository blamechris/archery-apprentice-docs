# Agentic LLM Workflow Experiment - Multi-Agent KMP Migration

**Status:** Active (Week 1 in progress)  
**Started:** 2025-10-18  
**Model:** Claude (Anthropic) via Claude Code CLI  
**Hypothesis:** Parallel LLM agents can accelerate complex migrations while maintaining quality  
**Tags:** #ai #llm #agentic-workflow #claude #experiment #methodology

---

## Experiment Overview

### Research Question
Can multiple parallel LLM agents, each with specialized contexts and missions, complete a complex architectural migration faster and with higher quality than traditional sequential development?

### Hypothesis
By orchestrating 3 specialized LLM agents working in Git worktrees on separate branches, we can:
1. **Reduce migration time** from 3+ weeks to ~1 week per major phase
2. **Maintain code quality** through isolated work and strategic merge ordering
3. **Prevent context loss** through explicit context handoff documents
4. **Enable parallel experimentation** without risk to production code

---

## Experimental Design

### Agent Architecture

**Agent O (Orchestrator)**
- **Role:** Coordination and context management
- **Context File:** `AGENT_O_ORCHESTRATOR.md`
- **Responsibilities:**
  - Track all 3 agent missions and progress
  - Resolve merge conflicts and coordination issues
  - Document critical decisions and lessons learned
  - Manage Week → Week transitions

**Agent 1 (AAP) - Platform Abstractions**
- **Role:** Remove platform dependencies from domain layer
- **Context File:** `AGENT_1_AAP.md`
- **Git Worktree:** `archery-agent-platform/`
- **Branch:** `kmp-migration/platform-abstractions`
- **Success Criteria:** Clean domain layer, no Android Context usage, 80% test coverage

**Agent 2 (AAM) - Module Setup**
- **Role:** Create KMP shared module structure
- **Context File:** `AGENT_2_AAM.md`
- **Git Worktree:** `archery-agent-modules/`
- **Branch:** `kmp-migration/module-setup`
- **Success Criteria:** 6 KMP modules configured, Room KMP integrated, documentation complete

**Agent 3 (AAA) - Code Analysis**
- **Role:** Analyze god classes and create refactoring plans
- **Context File:** `AGENT_3_AAA.md`
- **Git Worktree:** `archery-agent-analysis/`
- **Branch:** `kmp-migration/code-analysis`
- **Success Criteria:** Complete god class extraction roadmaps, ViewModel inventory

---

## Methodology: Context Handoff Protocol

### The Problem: LLM Context Loss
LLMs have no persistent memory between sessions. Without explicit context management, agents lose:
- Previous decisions and rationale
- Current progress and blockers
- Critical technical details
- Coordination state across agents

### The Solution: Markdown Context Files

Each agent gets a comprehensive markdown file containing:

**1. Mission & Status**
- Current week/phase
- Explicit success criteria
- Branch and worktree location

**2. Technical Context**
- Project architecture relevant to agent's mission
- Critical dependencies and constraints
- Known issues and workarounds

**3. Progress Tracking**
- Completed work with commit references
- In-progress tasks
- Blockers and dependencies on other agents

**4. Coordination State**
- What other agents are doing
- Merge order and dependencies
- Git worktree status

**5. Lessons Learned**
- Technical issues resolved
- Patterns discovered
- Decisions made and rationale

### Context Update Protocol
1. **Start of session:** Agent reads its context file
2. **During work:** Agent updates context in real-time
3. **End of session:** Agent commits context file updates
4. **Before resume:** Next session reads updated context

---

## Infrastructure: Git Worktrees

### Why Git Worktrees?

**Traditional Approach (Sequential):**
```
Developer: Work on feature A → Commit → Switch branch → Work on feature B
Problem: Context switching, can't compare branches side-by-side
```

**Docker Approach (Considered but rejected):**
```
Setup time: 2 hours
Learning curve: 3-4 hours
Complexity: Volume mounts, networking, image builds
Performance: Overhead from containerization
```

**Git Worktrees (Chosen):**
```
Setup time: 30 seconds
Learning curve: 10 minutes
Complexity: Native Git feature
Performance: No overhead
```

### Worktree Structure
```
StudioProjects/
├── archery-apprentice/           # Main repo (Agent O)
├── archery-agent-platform/       # Agent 1 worktree
├── archery-agent-modules/        # Agent 2 worktree
└── archery-agent-analysis/       # Agent 3 worktree
```

Each worktree is:
- A complete checkout of the repository
- On its own branch
- Isolated from other worktrees
- Can run tests/builds independently

### Setup Script
Created `setup-worktrees.sh` for one-command setup:
```bash
#!/bin/bash
git worktree add ../archery-agent-platform -b kmp-migration/platform-abstractions
git worktree add ../archery-agent-modules -b kmp-migration/module-setup
git worktree add ../archery-agent-analysis -b kmp-migration/code-analysis
```

---

## Coordination Mechanisms

### 1. Strategic Merge Ordering

**Problem:** Parallel agents create merge conflicts if not coordinated

**Solution:** Risk-based merge order
1. **Agent 3 first** - Documentation only (safest)
2. **Agent 2 second** - New modules (additive, no conflicts)
3. **Agent 1 last** - Modifies existing code (needs Agent 2's modules)

**Result:** Zero merge conflicts in Week 1

### 2. Explicit Dependencies

**Problem:** Agents don't know what other agents need

**Solution:** Document dependencies in context files
- Agent 1 depends on Agent 2's modules → Agent 1 merges after Agent 2
- Agent 3's analysis informs Agent 1 & 2's work → Agent 3 merges first
- All agents depend on each other's docs → Pull main after each merge

### 3. CI/CD as Quality Gate

**Problem:** How do we ensure parallel work maintains quality?

**Solution:** No merge without green CI/CD
- 80% test coverage enforced
- All tests must pass
- Agent can't merge until CI/CD passes

---

## Observations & Learnings

### Week 1 Learnings

**✅ What Worked Well:**

1. **Context Files Prevent Re-Work**
   - Agent 2 encountered Kotlin version conflict
   - Documented in AGENT_2_AAM.md with fix
   - Agent O saw the issue in context review
   - Prevented same mistake in Agent 1's work

2. **Git Worktrees Enable True Parallelism**
   - All 3 agents working simultaneously
   - No branch switching delays
   - Can compare changes side-by-side
   - Zero conflicts from isolated work

3. **Strategic Merge Order Eliminates Conflicts**
   - Documentation → Modules → Code changes
   - Each agent pulls main after upstream merge
   - Clean merge path for all 3 PRs

4. **Test Coverage as Quality Metric**
   - 80% threshold catches regressions
   - Agent 1 currently raising coverage
   - Tests validate platform abstractions work correctly

**⚠️ Challenges Encountered:**

1. **Initial Prompt Accuracy Critical**
   - Agent 1 prompt included Hilt (project uses manual DI)
   - Agent 2 prompt included Koin (project uses manual DI)
   - Had to correct both prompts mid-flight
   - **Lesson:** Validate prompts against actual architecture before agent starts

2. **Transitive Dependency Management**
   - Room KMP pulled Kotlin 2.1.20-dev
   - Project uses Kotlin 2.0.21
   - Caused ABI version incompatibility
   - **Lesson:** KMP libraries may force incompatible dependency versions

3. **Platform Abstraction Test Mocking**
   - Removing Context parameters broke 21+ tests
   - Tests assumed `PlatformProvider.isInitialized()` is true
   - Needed comprehensive mock strategy
   - **Lesson:** Platform abstractions require upfront test mock planning

4. **Worktree Isolation is Feature, Not Bug**
   - Agents can't see each other's uncommitted files
   - This is by design (prevents conflicts)
   - Requires strategic merge order for visibility
   - **Lesson:** Merge early and often to share context

---

## Metrics & Results

### Time Efficiency

**Traditional Sequential Approach (Estimated):**
- Platform abstractions: 1 week
- Module setup: 1 week  
- Code analysis: 1 week
- **Total: 3 weeks**

**Parallel Agent Approach (Actual):**
- All 3 tasks in parallel: ~3 days (2 merged, 1 in final CI/CD)
- **Total: 3 days (7x faster)**

### Quality Metrics

**Code Quality:**
- ✅ 80% test coverage maintained
- ✅ All CI/CD checks passing (2/3 PRs merged)
- ✅ Zero production bugs (work on branches)

**Documentation Quality:**
- ✅ 3 comprehensive context files (AGENT_1_AAP.md, AGENT_2_AAM.md, AGENT_3_AAA.md)
- ✅ 1 orchestrator context file (AGENT_O_ORCHESTRATOR.md)
- ✅ 3 technical decision documents (MODULE_ARCHITECTURE.md, DI_STRATEGY.md, GOD_CLASS_EXTRACTION_PLANS.md)

**Collaboration Efficiency:**
- ✅ Zero merge conflicts
- ✅ Clean Git history
- ✅ Strategic merge order worked as planned

---

## Agent Capabilities Discovered

### What LLM Agents Excel At

1. **Repetitive Pattern Application**
   - Agent 1 removed Context from 6 files using consistent pattern
   - Fixed 96 test occurrences with same mock strategy
   - Zero mistakes in repetitive refactoring

2. **Comprehensive Analysis**
   - Agent 3 catalogued 25 ViewModels in hours (would take days manually)
   - Identified all god class extraction opportunities
   - Created detailed extraction roadmaps

3. **Test Coverage Raising**
   - Agent 1 systematically adding tests to meet 80% threshold
   - Follows existing test patterns
   - Comprehensive coverage of edge cases

4. **Documentation Generation**
   - All 3 agents created detailed technical docs
   - Consistent markdown formatting
   - Clear explanations of decisions

### What LLM Agents Struggle With

1. **Architectural Creativity**
   - Agents default to common patterns (Hilt, Koin)
   - Needed human guidance to use existing manual DI pattern
   - **Mitigation:** Explicit architecture context in prompt

2. **Transitive Dependency Issues**
   - Agent 2 didn't anticipate Room KMP pulling incompatible Kotlin version
   - Required human debugging to identify root cause
   - **Mitigation:** Human review of gradle dependency trees

3. **Cross-Agent Coordination**
   - Agents don't automatically sync context across worktrees
   - Requires orchestrator (Agent O) to manage dependencies
   - **Mitigation:** Explicit coordination protocols and merge order

4. **Ambiguity Resolution**
   - When multiple valid approaches exist, agents need human decision
   - Example: Git worktrees vs Docker required human analysis
   - **Mitigation:** Agent O documents decisions for future reference

---

## Experimental Protocol Refinements

### For Week 2 (Planned)

1. **Pre-Flight Checklist**
   - [ ] Validate all agent prompts against actual architecture
   - [ ] Review transitive dependencies before agent starts
   - [ ] Create test mock strategy before removing dependencies
   - [ ] Document expected agent coordination points

2. **Enhanced Context Files**
   - Add "Dependencies on Other Agents" section
   - Include "Known Unknowns" section for ambiguities
   - Track "Human Decisions Required" separately

3. **Improved Merge Strategy**
   - Pull main more frequently (don't wait for full PR)
   - Consider feature flags for in-progress work
   - Smaller, more frequent merges

4. **Quality Gates**
   - Run subset of tests locally before pushing to CI/CD
   - Code coverage checks before creating PR
   - Architecture validation (no Android SDK in domain layer)

---

## Replication Guide

### For Others Running This Experiment

**Prerequisites:**
- Git repository with branches
- LLM with code generation capabilities (Claude, GPT-4, etc.)
- Ability to run tests locally
- CI/CD pipeline

**Step 1: Design Agent Missions**
- Identify 3 parallel, low-conflict tasks
- Ensure tasks are additive or isolated
- Plan strategic merge order

**Step 2: Create Git Worktrees**
```bash
git worktree add ../agent1-workspace -b agent1-branch
git worktree add ../agent2-workspace -b agent2-branch
git worktree add ../agent3-workspace -b agent3-branch
```

**Step 3: Create Context Files**
- One per agent with mission, context, progress
- One orchestrator file with coordination state
- Store in repo at `docs/AGENT_CONTEXTS/`

**Step 4: Run Agents in Parallel**
- Each agent works in its worktree
- Updates its context file as it works
- No cross-agent communication (isolated)

**Step 5: Orchestrate Merges**
- Merge in risk order (docs → new code → modifications)
- Each agent pulls main after upstream merge
- CI/CD required before merge

**Step 6: Document Learnings**
- Update context files with issues/solutions
- Track time savings vs sequential approach
- Note what agents excel at vs struggle with

---

## Future Research Questions

1. **Scaling:** Can this work with 5+ agents? What's the coordination overhead?
2. **Complexity:** What types of tasks are too complex for parallel agents?
3. **Context Limits:** How large can context files get before agents struggle?
4. **Human-in-Loop:** What's the optimal balance of automation vs human decisions?
5. **LLM Models:** Do different models (GPT-4, Claude, etc.) excel at different agent roles?

---

## Conclusion (Week 1)

The parallel agentic LLM workflow has proven highly effective for the KMP migration:
- **7x faster** than sequential approach (3 days vs 3 weeks estimated)
- **Zero merge conflicts** through strategic coordination
- **Maintained quality** via 80% test coverage and CI/CD gates
- **Clear documentation** of all decisions and learnings

The key insight is that **coordination overhead is worth the parallelism gains** when:
1. Tasks are naturally parallel (low conflict zones)
2. Context files prevent work duplication
3. Strategic merge order minimizes conflicts
4. Quality gates ensure standards are met

This is an ongoing experiment - Week 2 will test whether the protocol scales to more complex code migration tasks.

---

**Last Updated:** 2025-10-19  
**Next Review:** Week 2 kickoff (after agent methodology refinements)  
**Status:** Experiment ongoing, hypothesis supported by Week 1 results



## Week 1 Critical Discoveries (2025-10-19)

### Discovery 1: Coverage Threshold Not Actually Enforced
**Finding:** Documentation claimed "80% coverage enforced in CI/CD" but investigation revealed:
- CI/CD runs coverage reports but doesn't fail on low coverage
- All coverage steps have `continue-on-error: true` or `fail_ci_if_error: false`
- No JaCoCo violation rules configured in gradle
- 80% threshold is aspirational, not enforced

**Impact on Experiment:**
- Agent 1 worked hard to reach 80% when it could have merged at 70%
- Documentation mismatch created false constraint
- Shows importance of verifying documented constraints

**Lesson:** Always verify enforcement mechanisms match documentation claims

### Discovery 2: Detekt Not Configured Despite Documentation
**Finding:** CLAUDE.md mentions detekt for code complexity analysis, but:
- Detekt is not configured anywhere in the project
- Not in gradle files, CI workflow, or configuration files
- No automated complexity monitoring exists

**Impact on Experiment:**
- User expected to see detekt results on PR
- Another documentation/reality mismatch
- Created expectation that couldn't be met

**Lesson:** Audit tooling claims in documentation against actual project setup

### Discovery 3: Context Drift Without Sync Protocol
**Finding:** Agent 1's context said "Week 1 complete, awaiting merge" when reality was "70% coverage, work ongoing"

**Root Cause:** No protocol for updating agent contexts when status changes

**Solution Created:** 
- Created `agent-context-sync` skill
- Orchestrator leaves update files for agents
- Agents check for updates at session start
- Respects agent boundaries (each agent owns its context)

**Impact:** Prevents future context drift, enables accurate agent resumption

---

## Experimental Protocol Updates (2025-10-19)

### New: Agent Context Sync Protocol
**File:** `.claude/skills/agent-context-sync/SKILL.md`

**How It Works:**
1. Orchestrator discovers agent's context is stale
2. Orchestrator creates `AGENT_[N]_CONTEXT_UPDATE_NEEDED.md` with update instructions
3. Agent checks for update files at session start
4. Agent applies updates to its own context file
5. Agent deletes update file after processing

**Benefits:**
- Respects agent boundaries (no direct editing)
- Async coordination (leave notes for agents)
- Prevents context drift
- Documented communication protocol

### Week 2 Pre-Flight Checklist Enhancement

**Added to protocol:**
- [ ] Verify documented constraints are actually enforced (coverage, complexity, etc.)
- [ ] Audit tooling claims against actual project configuration
- [ ] Check agent contexts for drift before starting new work
- [ ] Use agent-context-sync skill for all coordination updates

---

## Metrics Update (Week 1 Extended)

### Time Efficiency (Revised)
**Original Estimate:** 3 days for Week 1  
**Actual:** 5-6 days (coverage work extended Week 1)

**Breakdown:**
- Agent 3: 2 days ✅ (on estimate)
- Agent 2: 3 days ✅ (on estimate, including Kotlin version conflict)
- Agent 1: 5-6 days ⚠️ (expected 3 days, coverage work took longer)

**Revised Multiplier:** 5-6x faster than sequential (not 7x) when accounting for coverage extension

### Quality Discoveries
**Documentation Accuracy:** ⚠️ 
- 2 major documentation/reality mismatches discovered
- Coverage enforcement: Claimed but not implemented
- Detekt analysis: Claimed but not configured

**Lesson for Experiment:** Document verification is critical before agent work begins

---



## Week 1 Completion (2025-10-20)

### Final Results

**Timeline:**
- **Planned:** 3 days
- **Actual:** 7 days (Oct 18-20)
- **Multiplier:** 2.3x slower than expected (vs 7x faster than sequential)

**PRs Merged:**
1. ✅ Agent 3 (AAA) - God class analysis (2 days) - ON SCHEDULE
2. ✅ Agent 2 (AAM) - KMP modules (3 days) - ON SCHEDULE
3. ✅ PR #131 - Orchestrator docs (1 day)
4. ✅ Agent 1 (AAP) - Platform abstractions (7 days) - EXTENDED

**Breakdown:**
- Agent 3: 2 days ✅ (estimate accurate)
- Agent 2: 3 days ✅ (estimate accurate)  
- Agent 1: 7 days ⚠️ (expected 3, took 7 due to coverage work)

**Why Agent 1 took longer:**
- Coverage enforcement not actually enabled (discovery time)
- JaCoCo tooling limitations (analysis time)
- 11 lines appeared "uncovered" despite having tests
- Comprehensive analysis to prove code quality (2 days)
- Configuration adjustments and documentation (1 day)

---

### Experiment Learnings - Revised

**Time Efficiency (Updated):**
- **Sequential estimate:** 3 weeks (15 working days)
- **Parallel actual:** 7 days
- **True multiplier:** 2.1x faster (not 7x as initially claimed)
- **Realistic expectation:** Parallel reduces time by 50-70%, not 85%+

**Why slower than expected:**
- Coverage work is hard to estimate (10% gap ≠ 1 day)
- Tool limitations take time to discover and document
- Context coordination overhead (worktree isolation, update files)
- Unknowns compound (coverage + JaCoCo + Copilot feedback)

---

### Critical Discovery #3: Coverage Work Takes Longer Than Expected

**Finding:** 10% coverage gap ≠ 1 day of work

**Agent 1's Coverage Journey:**
1. Day 1-3: Platform abstractions (code complete)
2. Day 4: Discover coverage not enforced
3. Day 5: Enable enforcement, discover 70% actual
4. Day 6: Attempt to raise to 80%, discover JaCoCo limitations
5. Day 7: Comprehensive analysis, prove tests exist, adjust thresholds

**Lesson:** Coverage work is discovery-heavy:
- Must investigate WHY lines are uncovered
- Tooling limitations require analysis
- Can't just "write tests" without understanding root cause
- Documentation and justification take time

---

### Critical Discovery #4: JaCoCo Tooling Limitations

**Finding:** Well-tested code can show as "uncovered" in JaCoCo reports

**What JaCoCo+Robolectric Can't Measure:**
- Reflection-invoked methods (ViewModel lifecycle)
- Empty method bodies (no-op cleanup)
- Field initializers with Robolectric
- Async Flow operators (coroutine closures)

**Impact on Experiment:**
- 2 days analyzing why 11 lines showed as uncovered
- Comprehensive proof that tests actually exist
- Decision to adjust patch coverage threshold (60% realistic)
- Research into alternatives (Kover) for Week 2+

**Lesson:** Trust but verify tooling claims. Coverage numbers can be misleading.

---

### What Worked (Updated for Week 1 Completion)

1. **Git Worktrees** ✅
   - Zero merge conflicts from parallel work
   - Clean isolation between agents
   - Easy side-by-side comparison

2. **Context Files** ✅
   - Prevented massive context loss
   - Enabled session resumption
   - Documented critical decisions
   - **Caveat:** Must be kept up-to-date or drift occurs

3. **Strategic Merge Order** ✅
   - Agent 3 (docs) → Agent 2 (modules) → Agent 1 (code changes)
   - No conflicts from this ordering
   - Smooth integration path

4. **Copilot Integration** ✅
   - Caught documentation contradictions
   - Flagged code quality improvements
   - 8 issues total (2 critical fixed, 6 deferred)

5. **Agent Context Sync Protocol** ✅ NEW
   - Created to solve context drift
   - Orchestrator leaves update files for agents
   - Agents check at session start
   - **Limitation:** Worktree isolation requires commit or copy

---

### What Didn't Work (Updated)

1. **Initial Time Estimates** ❌
   - Expected 3 days, took 7 days
   - Coverage work underestimated by 4 days
   - Lesson: Add buffer for unknowns (coverage, tooling, discovery)

2. **Coverage Verification Assumptions** ❌
   - Assumed 80% enforcement was working (it wasn't)
   - Assumed JaCoCo was reliable (it has limitations)
   - 2 days lost to discovering and documenting these issues

3. **Worktree Update File Distribution** ⚠️
   - Update files in main repo not visible to agent worktrees
   - Required workaround (commit first)
   - Added friction to coordination

4. **Context Drift Without Sync** ⚠️
   - Agent 1's context said "complete" when actually 70%
   - Created agent-context-sync protocol to fix
   - Must be diligent about keeping contexts current

---

### Recommendations for Week 2

**Time Estimation:**
1. **Add 50% buffer** for coverage-related work
2. **Verify tooling claims** before starting (don't trust docs)
3. **Plan for unknowns** - every agent will hit something unexpected
4. **Realistic multiplier:** 2-3x faster than sequential, not 5-7x

**Coverage Strategy:**
1. **Check current coverage** before estimating work
2. **Investigate uncovered lines** before writing tests
3. **Document tooling limitations** when discovered
4. **Consider alternatives** (Kover) for better Kotlin support

**Coordination:**
1. **Commit update files** before agents resume (worktree isolation)
2. **Keep contexts current** (context drift creates confusion)
3. **Verify constraints** match reality (coverage, detekt, etc.)
4. **Smaller, more frequent merges** (don't batch all work to end)

---

### Metrics Update (Week 1 Final)

**Quality Maintained:** ✅
- 80% project coverage target set
- All tests passing
- Platform abstractions well-tested (JaCoCo limitations documented)
- Zero production bugs (work on branches)

**Time Efficiency (Revised):**
- **Sequential estimate:** 15 days (3 weeks)
- **Parallel actual:** 7 days
- **Speedup:** 2.1x faster
- **More realistic than initial 7x claim**

**Collaboration Efficiency:** ✅
- Zero merge conflicts
- Clean git history
- Strategic merge order worked
- Context files enabled resumption

**Documentation Quality:** ✅
- 9 issues documented (7-9 in orchestrator context)
- Tech debt tracker with 5 items
- Copilot feedback tracked
- Week 1 lessons captured

---

### Hypothesis Update

**Original Hypothesis:**
> Parallel LLM agents can complete complex migration 5-7x faster

**Revised Hypothesis (After Week 1):**
> Parallel LLM agents can complete complex migration 2-3x faster when:
> - Tasks are naturally parallel (low conflict zones)
> - Unknowns are factored into estimates (coverage, tooling, discovery)
> - Context coordination overhead is managed (update files, merge frequency)
> - Quality gates ensure standards (tests, coverage, reviews)

**Conclusion:** Still valuable (2x faster is significant), but more realistic expectations needed.

---

